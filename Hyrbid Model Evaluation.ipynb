{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac9de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Tracks and Playlists\n",
    "\n",
    "from py2neo import Graph, Node, Relationship\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "uri = \"...\"\n",
    "username = \"...\"\n",
    "password = \"...\"\n",
    "graph = Graph(uri, auth=(username, password))\n",
    "\n",
    "# Retrieve track nodes and their properties\n",
    "track_query = '''\n",
    "MATCH (t:Track)\n",
    "RETURN t.acousticness, t.album_id, t.artist_ids, t.danceability, t.duration_ms, t.energy, t.explicit,\n",
    "       t.genre, t.id, t.key, t.liveness, t.loudness, t.mode, t.popularity, t.speechiness, t.tempo, t.valence\n",
    "'''\n",
    "\n",
    "track_result = graph.run(track_query).data()\n",
    "\n",
    "# Create dict for tracks\n",
    "track_dict = {row['t.id']: {\n",
    "    'index': idx,\n",
    "    'acousticness': row['t.acousticness'],\n",
    "    'album_id': row['t.album_id'],\n",
    "    'artist_ids': row['t.artist_ids'],\n",
    "    'danceability': row['t.danceability'],\n",
    "    'duration_ms': row['t.duration_ms'],\n",
    "    'energy': row['t.energy'],\n",
    "    'explicit': row['t.explicit'],\n",
    "    'genre': row['t.genre'],\n",
    "    'liveness': row['t.liveness'],\n",
    "    'loudness': row['t.loudness'],\n",
    "    'popularity': row['t.popularity'],\n",
    "    'speechiness': row['t.speechiness'],\n",
    "    'tempo': row['t.tempo'],\n",
    "    'valence': row['t.valence'],\n",
    "} for idx, row in enumerate(track_result)}\n",
    "\n",
    "\n",
    "# Retrieve playlist nodes and their properties\n",
    "playlist_query = '''\n",
    "MATCH (p:Playlist)-[:CONTAINS]->(t:Track)\n",
    "WITH p.playlist_id AS playlist_id, collect(t.id) AS tracklist\n",
    "RETURN playlist_id, tracklist\n",
    "'''\n",
    "\n",
    "playlist_result = graph.run(playlist_query).data()\n",
    "\n",
    "# Create playlists dictionary\n",
    "playlist_dict = {row['playlist_id']: {\n",
    "    'index': idx,\n",
    "    'track_ids': row['tracklist']\n",
    "} for idx, row in enumerate(playlist_result)}\n",
    "\n",
    "# Normalize Continuous Features\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "continuous_features = [\"acousticness\", \"danceability\", \"energy\", \"liveness\", \"loudness\", \"popularity\", \"speechiness\", \"tempo\", \"valence\"]\n",
    "scalers = {}\n",
    "\n",
    "for feature in continuous_features:\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_values = np.array([track[feature] for track in track_dict.values()]).reshape(-1, 1)\n",
    "    scaler.fit(feature_values)\n",
    "    scalers[feature] = scaler\n",
    "\n",
    "    for track in track_dict.values():\n",
    "        track[feature] = scaler.transform([[track[feature]]])[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4d860fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing Relationship Data\n",
    "\n",
    "n_playlists = len(playlist_dict)\n",
    "n_tracks = len(track_dict)\n",
    "\n",
    "# CONTAINS relationship (playlist-track)\n",
    "contains_query = '''\n",
    "MATCH (p:Playlist)-[:CONTAINS]->(t:Track)\n",
    "RETURN p.playlist_id AS playlist_id, t.id AS track_id\n",
    "'''\n",
    "contains_result = graph.run(contains_query).data()\n",
    "\n",
    "# Create CONTAINS list (Should be playlist_id and track_id pairs)\n",
    "contains_list = [(record['playlist_id'], record['track_id']) for record in contains_result]\n",
    "\n",
    "# SHARED_ALBUM, SHARED_ARTIST, and SHARED_GENRE lists (tuple array should be track1_id and track2_id)\n",
    "album_query = '''\n",
    "    MATCH (t1:Track)-[:SHARED_ALBUM]->(t2:Track)\n",
    "    RETURN t1.id AS track1_id, t2.id AS track2_id\n",
    "    '''\n",
    "album_result = graph.run(album_query).data()\n",
    "album_list = [(record['track1_id'], record['track2_id']) for record in album_result]\n",
    "\n",
    "artist_query = '''\n",
    "    MATCH (t1:Track)-[:SHARED_ARTIST]->(t2:Track)\n",
    "    RETURN t1.id AS track1_id, t2.id AS track2_id\n",
    "    '''\n",
    "artist_result = graph.run(album_query).data()\n",
    "artist_list = [(record['track1_id'], record['track2_id']) for record in artist_result]\n",
    "\n",
    "genre_query = '''\n",
    "    MATCH (t1:Track)-[:SHARED_GENRE]->(t2:Track)\n",
    "    RETURN t1.id AS track1_id, t2.id AS track2_id\n",
    "    '''\n",
    "genre_result = graph.run(album_query).data()\n",
    "genre_list = [(record['track1_id'], record['track2_id']) for record in genre_result]\n",
    "\n",
    "\n",
    "# COSINE_SIMILARITY relationship (track-track)\n",
    "# tuple array will be (track1_id, track2_id, similarity_value)\n",
    "cosine_similarity_query = '''\n",
    "MATCH (t1:Track)-[r:COSINE_SIMILARITY]->(t2:Track)\n",
    "RETURN t1.id AS track1_id, t2.id AS track2_id, r.value AS similarity_value\n",
    "'''\n",
    "cosine_similarity_result = graph.run(cosine_similarity_query).data()\n",
    "cosine_list = [(record['track1_id'], record['track2_id'], record['similarity_value']) for record in cosine_similarity_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c14e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_similarity_matrix = np.zeros((n_tracks, n_tracks))\n",
    "\n",
    "for track1_id, track2_id, similarity_value in cosine_list:\n",
    "    track1_idx = track_dict[track1_id]['index']\n",
    "    track2_idx = track_dict[track2_id]['index']\n",
    "    track_similarity_matrix[track1_idx][track2_idx] = similarity_value*0.8\n",
    "    track_similarity_matrix[track2_idx][track1_idx] = similarity_value*0.8\n",
    "    \n",
    "for track1_id, track2_id in album_list:\n",
    "    track1_idx = track_dict[track1_id]['index']\n",
    "    track2_idx = track_dict[track2_id]['index']\n",
    "    track_similarity_matrix[track1_idx][track2_idx] += 0.50\n",
    "    track_similarity_matrix[track2_idx][track1_idx] += 0.50\n",
    "    \n",
    "for track1_id, track2_id in artist_list:\n",
    "    track1_idx = track_dict[track1_id]['index']\n",
    "    track2_idx = track_dict[track2_id]['index']\n",
    "    track_similarity_matrix[track1_idx][track2_idx] += 0.30\n",
    "    track_similarity_matrix[track2_idx][track1_idx] += 0.30\n",
    "    \n",
    "for track1_id, track2_id in genre_list:\n",
    "    track1_idx = track_dict[track1_id]['index']\n",
    "    track2_idx = track_dict[track2_id]['index']\n",
    "    track_similarity_matrix[track1_idx][track2_idx] += 0.75\n",
    "    track_similarity_matrix[track2_idx][track1_idx] += 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b499fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percision: 0.18179999999999993\n",
      "Recall: 0.1819999999999999\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "def playlist_vector(playlist_id):\n",
    "    track_ids = playlist_dict[playlist_id]['track_ids']\n",
    "    return np.mean([track_similarity_matrix[track_dict[track_id]['index']] for track_id in track_ids], axis=0)\n",
    "\n",
    "def find_similar_playlists(playlist_id, k=10):\n",
    "    track_ids = playlist_dict[playlist_id]['track_ids']\n",
    "    random.shuffle(track_ids)\n",
    "    track_ids_train, track_ids_test = train_test_split(track_ids,test_size=0.5)\n",
    "    train_playlist_vector = np.mean([track_similarity_matrix[track_dict[track_id]['index']] for track_id in track_ids_train], axis=0)\n",
    "    test_playlist_vector = np.mean([track_similarity_matrix[track_dict[track_id]['index']] for track_id in track_ids_test], axis=0)\n",
    "    \n",
    "    similarities = []\n",
    "    \n",
    "    train_test_similarity = np.dot(train_playlist_vector, test_playlist_vector)\n",
    "    similarities.append((\"OTHER_HALF\", train_test_similarity))\n",
    "                        \n",
    "    for other_playlist_id, other_playlist_data in playlist_dict.items():\n",
    "        if other_playlist_id == playlist_id: # EDGE CASE THEY'RE THE SAME\n",
    "            continue\n",
    "\n",
    "        other_playlist_vector = playlist_vector(other_playlist_id)\n",
    "        similarity = np.dot(train_playlist_vector, other_playlist_vector)\n",
    "        similarities.append((other_playlist_id, similarity))\n",
    "\n",
    "    similar_playlists = heapq.nlargest(k, similarities, key=lambda x: x[1])\n",
    "    return similar_playlists, track_ids_train, track_ids_test\n",
    "\n",
    "def cf_recommendation(playlist_id, k=10):\n",
    "    similar_playlists, track_ids_MAIN, track_ids_OTHER_HALF = find_similar_playlists(playlist_id, k)\n",
    "    track_scores = defaultdict(float)\n",
    "\n",
    "    for playlist_id, similarity in similar_playlists:\n",
    "        if playlist_id == \"OTHER_HALF\":\n",
    "            for track_id in track_ids_OTHER_HALF:\n",
    "                track_scores[track_id] += similarity\n",
    "        else:\n",
    "            for track_id in playlist_dict[playlist_id]['track_ids']:\n",
    "                track_scores[track_id] += similarity\n",
    "\n",
    "    sorted_scores = heapq.nlargest(k, track_scores.items(), key=lambda x: x[1])\n",
    "    recommended_track_ids = [[track_id, score] for track_id, score in sorted_scores if track_id not in playlist_dict[playlist_id]['track_ids']]\n",
    "    return recommended_track_ids, track_ids_MAIN, track_ids_OTHER_HALF\n",
    "\n",
    "def cbf_recommendation(track_ids, track_similarity_matrix, k=10):\n",
    "    #track_ids = playlist_dict[playlist_id]['track_ids']\n",
    "    playlist_track_matrix = track_similarity_matrix[[track_dict[track_id]['index'] for track_id in track_ids]]\n",
    "    mean_similarity = np.mean(playlist_track_matrix, axis=0)\n",
    "    top_indices = heapq.nlargest(k+len(track_ids), range(len(mean_similarity)), key=lambda i: mean_similarity[i])\n",
    "    recommended_indices = [i for i in top_indices if i not in [track_dict[track_id]['index'] for track_id in track_ids]][:k]\n",
    "    recommended_tracks = [(track_result[i]['t.id'], mean_similarity[i]) for i in recommended_indices]\n",
    "    return recommended_tracks\n",
    "\n",
    "def hybrid_recommendation(playlist_id, track_similarity_matrix, k=10, cf_weight=0.5):\n",
    "    cf_recommendations, track_ids_MAIN, track_ids_OTHER_HALF = cf_recommendation(playlist_id, k)\n",
    "    cbf_recommendations = cbf_recommendation(track_ids_MAIN, track_similarity_matrix, k)\n",
    "\n",
    "    track_scores = defaultdict(float)\n",
    "    for track_id, score in cf_recommendations:\n",
    "        track_scores[track_id] += cf_weight * score\n",
    "    for track_id, score in cbf_recommendations:\n",
    "        track_scores[track_id] += (1 - cf_weight) * score\n",
    "\n",
    "    sorted_scores = heapq.nlargest(k, track_scores.items(), key=lambda x: x[1])\n",
    "    recommended_track_ids = [track_id for track_id, score in sorted_scores]\n",
    "    return recommended_track_ids, track_ids_OTHER_HALF\n",
    "\n",
    "def test(sample):\n",
    "    percisionKs = []\n",
    "    recallKs = []\n",
    "    for i in range(0, sample):\n",
    "        target_playlist_id = random.choice(list(playlist_dict.items()))[0]\n",
    "        recommended_tracks, relavant_tracks = hybrid_recommendation(target_playlist_id, track_similarity_matrix, k=50, cf_weight=0.3)\n",
    "        count = 0\n",
    "        for recommendation in recommended_tracks:\n",
    "            if recommendation in relavant_tracks:\n",
    "                count += 1\n",
    "        percisionK = count / len(recommended_tracks)\n",
    "        recallK = count / len(relavant_tracks)\n",
    "        percisionKs.append(percisionK)\n",
    "        recallKs.append(recallK)\n",
    "    avg_percisionK = sum(percisionKs) / len(percisionKs)\n",
    "    avg_recallK = sum(recallKs) / len(recallKs)\n",
    "    return avg_percisionK, avg_recallK, percisionKs, recallKs\n",
    "\n",
    "avg_percisionK, avg_recallK, percisionKs, recallKs = test(100) # <= The number of playlists to sample\n",
    "print(\"Percision:\", str(avg_percisionK))\n",
    "print(\"Recall:\", str(avg_recallK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe309cec-f7bb-417f-9f73-24d046f26453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# precision@k\n",
    "precision50 = 0.1911999999999999\n",
    "precision50 = 0.22959999999999994\n",
    "precision50 = 0.2483999999999999\n",
    "precision100 = 0.14579999999999996\n",
    "precision100 = 0.13529999999999995\n",
    "precision100 = 0.1595999999999999\n",
    "precision200 = 0.11895000000000001\n",
    "precision200 = 0.12045000000000003\n",
    "precision200 = 0.10065000000000002\n",
    "\n",
    "# recall@k\n",
    "recall50 = 0.18755555555555553\n",
    "recall50 = 0.20679999999999996\n",
    "recall50 = 0.26199999999999996\n",
    "recall100 = 0.3552818713450292\n",
    "recall100 = 0.3183999999999999\n",
    "recall100 = 0.24699999999999991\n",
    "recall200 = 0.4711999999999999\n",
    "recall200 = 0.41139999999999977\n",
    "recall200 = 0.4071263157894736"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

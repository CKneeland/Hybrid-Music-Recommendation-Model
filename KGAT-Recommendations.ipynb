{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5821a2-eb84-41d4-81e6-9bb91153e010",
   "metadata": {},
   "source": [
    "# Retrieve Graph\n",
    "\n",
    "#### Node Dictionaries\n",
    "+ track_dict\n",
    "+ playlist_dict\n",
    "\n",
    "#### Edge Dictionaries\n",
    "+ shared_album_edges\n",
    "+ shared_artist_edges\n",
    "+ shared_genre_edges\n",
    "+ cosine_similarity_edges\n",
    "+ contains_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700e65e6-7612-44b2-878f-da250644ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "uri = \"...\"\n",
    "username = \"...\"\n",
    "password = \"...\"\n",
    "graph = Graph(uri, auth=(username, password))\n",
    "\n",
    "# Retrieve track nodes and their properties\n",
    "track_query = '''\n",
    "MATCH (t:Track)\n",
    "RETURN t.acousticness, t.album_id, t.artist_ids, t.danceability, t.duration_ms, t.energy, t.explicit,\n",
    "       t.genre, t.id, t.key, t.liveness, t.loudness, t.mode, t.popularity, t.speechiness, t.tempo, t.valence\n",
    "'''\n",
    "\n",
    "track_result = graph.run(track_query).data()\n",
    "\n",
    "# Create dictionary for tracks\n",
    "track_dict = {row['t.id']: {\n",
    "    'acousticness': row['t.acousticness'],\n",
    "    'album_id': row['t.album_id'],\n",
    "    'artist_ids': row['t.artist_ids'],\n",
    "    'danceability': row['t.danceability'],\n",
    "    'duration_ms': row['t.duration_ms'],\n",
    "    'energy': row['t.energy'],\n",
    "    'explicit': row['t.explicit'],\n",
    "    'genre': row['t.genre'],\n",
    "    'key': row['t.key'],\n",
    "    'liveness': row['t.liveness'],\n",
    "    'loudness': row['t.loudness'],\n",
    "    'mode': row['t.mode'],\n",
    "    'popularity': row['t.popularity'],\n",
    "    'speechiness': row['t.speechiness'],\n",
    "    'tempo': row['t.tempo'],\n",
    "    'valence': row['t.valence'],\n",
    "} for row in track_result}\n",
    "\n",
    "# Retrieve playlist nodes and their properties\n",
    "playlist_query = '''\n",
    "MATCH (p:Playlist)\n",
    "RETURN p.playlist_id, p.tracklist\n",
    "'''\n",
    "\n",
    "playlist_result = graph.run(playlist_query).data()\n",
    "\n",
    "# Create dictionary playlists\n",
    "playlist_dict = {row['p.playlist_id']: row['p.tracklist'] for row in playlist_result}\n",
    "\n",
    "# Create defaultdicts for relationships\n",
    "shared_album_edges = defaultdict(list)\n",
    "shared_artist_edges = defaultdict(list)\n",
    "shared_genre_edges = defaultdict(list)\n",
    "cosine_similarity_edges = defaultdict(list)\n",
    "contains_edges = defaultdict(list)\n",
    "\n",
    "# Retrieve SHARED_ALBUM relationships\n",
    "shared_album_query = '''\n",
    "MATCH (t1:Track)-[:SHARED_ALBUM]->(t2:Track)\n",
    "RETURN t1.id, t2.id\n",
    "'''\n",
    "shared_album_result = graph.run(shared_album_query).data()\n",
    "\n",
    "for row in shared_album_result:\n",
    "    track1_id, track2_id = row['t1.id'], row['t2.id']\n",
    "    shared_album_edges[track1_id].append(track2_id)\n",
    "    shared_album_edges[track2_id].append(track1_id)\n",
    "\n",
    "# Retrieve SHARED_ARTIST relationships\n",
    "shared_artist_query = '''\n",
    "MATCH (t1:Track)-[:SHARED_ARTIST]->(t2:Track)\n",
    "RETURN t1.id, t2.id\n",
    "'''\n",
    "shared_artist_result = graph.run(shared_artist_query).data()\n",
    "\n",
    "for row in shared_artist_result:\n",
    "    track1_id, track2_id = row['t1.id'], row['t2.id']\n",
    "    shared_artist_edges[track1_id].append(track2_id)\n",
    "    shared_artist_edges[track2_id].append(track1_id)\n",
    "\n",
    "# Retrieve SHARED_GENRE relationships\n",
    "shared_genre_query = '''\n",
    "MATCH (t1:Track)-[:SHARED_GENRE]->(t2:Track)\n",
    "RETURN t1.id, t2.id\n",
    "'''\n",
    "shared_genre_result = graph.run(shared_genre_query).data()\n",
    "\n",
    "for row in shared_genre_result:\n",
    "    track1_id, track2_id = row['t1.id'], row['t2.id']\n",
    "    shared_genre_edges[track1_id].append(track2_id)\n",
    "    shared_genre_edges[track2_id].append(track1_id)\n",
    "    \n",
    "# Retrieve COSINE_SIMILARITY relationships\n",
    "cosine_similarity_query = '''\n",
    "MATCH (t1:Track)-[r:COSINE_SIMILARITY]->(t2:Track)\n",
    "RETURN t1.id, t2.id, r.value\n",
    "'''\n",
    "cosine_similarity_result = graph.run(cosine_similarity_query).data()\n",
    "\n",
    "for row in cosine_similarity_result:\n",
    "    track1_id, track2_id, similarity = row['t1.id'], row['t2.id'], row['r.value']\n",
    "    cosine_similarity_edges[track1_id].append(track2_id)\n",
    "    cosine_similarity_edges[track2_id].append(track1_id)\n",
    "\n",
    "# Retrieve CONTAINS relationships\n",
    "contains_query = '''\n",
    "MATCH (p:Playlist)-[:CONTAINS]->(t:Track)\n",
    "RETURN p.playlist_id, t.id\n",
    "'''\n",
    "contains_result = graph.run(contains_query).data()\n",
    "\n",
    "for row in contains_result:\n",
    "    playlist_id, track_id = row['p.playlist_id'], row['t.id']\n",
    "    contains_edges[playlist_id].append(track_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a2b0f-e812-4a37-ac66-525546a8edc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating Adjacency Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe158dd7-b9ad-4c3f-bf94-46b5af815ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# Get unique track IDs\n",
    "unique_track_ids = list(track_dict.keys())\n",
    "\n",
    "# Create a track ID to index mapping\n",
    "track_id_to_index = {track_id: index for index, track_id in enumerate(unique_track_ids)}\n",
    "\n",
    "#print(track_id_to_index)\n",
    "\n",
    "# Get unique playlist IDs\n",
    "unique_playlist_ids = list(playlist_dict.keys())\n",
    "\n",
    "# Create a playlist ID to index mapping\n",
    "playlist_id_to_index = {playlist_id: index for index, playlist_id in enumerate(unique_playlist_ids)}\n",
    "\n",
    "#print(playlist_id_to_index)\n",
    "\n",
    "# Create adjacency matrices for each relationship\n",
    "n_tracks = len(unique_track_ids)\n",
    "shared_album_adj = lil_matrix((n_tracks, n_tracks), dtype=np.float32)\n",
    "shared_artist_adj = lil_matrix((n_tracks, n_tracks), dtype=np.float32)\n",
    "shared_genre_adj = lil_matrix((n_tracks, n_tracks), dtype=np.float32)\n",
    "cosine_similarity_adj = lil_matrix((n_tracks, n_tracks), dtype=np.float32)\n",
    "\n",
    "# Fill in the shared_album_adj matrix\n",
    "for track_id, connected_track_ids in shared_album_edges.items():\n",
    "    track_index = track_id_to_index[track_id]\n",
    "    for connected_track_id in connected_track_ids:\n",
    "        connected_track_index = track_id_to_index[connected_track_id]\n",
    "        shared_album_adj[track_index, connected_track_index] = .25\n",
    "\n",
    "# Fill in the shared_artist_adj matrix\n",
    "for track_id, connected_track_ids in shared_artist_edges.items():\n",
    "    track_index = track_id_to_index[track_id]\n",
    "    for connected_track_id in connected_track_ids:\n",
    "        connected_track_index = track_id_to_index[connected_track_id]\n",
    "        shared_artist_adj[track_index, connected_track_index] = .25\n",
    "\n",
    "# Fill in the shared_genre_adj matrix\n",
    "for track_id, connected_track_ids in shared_genre_edges.items():\n",
    "    track_index = track_id_to_index[track_id]\n",
    "    for connected_track_id in connected_track_ids:\n",
    "        connected_track_index = track_id_to_index[connected_track_id]\n",
    "        shared_genre_adj[track_index, connected_track_index] = .5\n",
    "\n",
    "# Fill in the cosine_similarity_adj matrix\n",
    "for track_id, connected_track_tuples in cosine_similarity_edges.items():\n",
    "    track_index = track_id_to_index[track_id]\n",
    "    for connected_track_id in connected_track_tuples:\n",
    "        connected_track_index = track_id_to_index[connected_track_id]\n",
    "        cosine_similarity_adj[track_index, connected_track_index] = .95\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade80f53-a695-40fc-ae74-3a199740ddf3",
   "metadata": {},
   "source": [
    "# KGAT Model Implementation: PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe7ef27-4ed3-45d7-945a-9bb5654a3cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "class KGATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(KGATLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        #self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.weight = nn.Parameter(torch.randn(in_features, out_features) * 0.1)  # multiply by 0.1 to increase values\n",
    "        self.reset_parameters()\n",
    "\n",
    "#    def reset_parameters(self):\n",
    "#        gain = nn.init.calculate_gain('relu') * math.sqrt(3)\n",
    "#        nn.init.uniform_(self.weight, -10, 10)\n",
    "#        self.weight.data.mul_(gain)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #print(self.weight)\n",
    "        #nn.Parameter(torch.randn(11, out_features) * 0.1)  # multiply by 0.1 to increase values\n",
    "        nn.init.kaiming_uniform_(self.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #print(self.weight)\n",
    "\n",
    "    def forward(self, adjacency_matrix, input_features):\n",
    "        output_features = torch.mm(input_features, self.weight)\n",
    "        return torch.mm(adjacency_matrix, output_features)\n",
    "\n",
    "class KGAT(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, num_layers):\n",
    "        super(KGAT, self).__init__()\n",
    "        self.input_bn = nn.BatchNorm1d(hidden_features)\n",
    "        self.hidden_bns = nn.ModuleList([nn.BatchNorm1d(hidden_features) for _ in range(num_layers - 1)])\n",
    "        self.in_features = in_features\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the input layer\n",
    "        self.input_layer = KGATLayer(in_features, hidden_features)\n",
    "\n",
    "        # Define the hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.hidden_layers.append(KGATLayer(hidden_features, hidden_features))\n",
    "\n",
    "        # Define the output layer\n",
    "        self.output_layer = KGATLayer(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, adjacency_matrix, input_features):\n",
    "        x = self.input_layer(adjacency_matrix, input_features)\n",
    "        x = x.unsqueeze(2)  # Add an extra dimension for Batch Normalization\n",
    "        x = F.relu(self.input_bn(x))\n",
    "        x = x.squeeze(2)  # Remove the extra dimension\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            x = layer(adjacency_matrix, x)\n",
    "            x = x.unsqueeze(2)  # Add an extra dimension for Batch Normalization\n",
    "            x = F.relu(self.hidden_bns[i](x))\n",
    "            x = x.squeeze(2)  # Remove the extra dimension\n",
    "\n",
    "        # Pass through the output layer\n",
    "        x = self.output_layer(adjacency_matrix, x)\n",
    "\n",
    "        return x        \n",
    "\n",
    "hidden_features = 64  # Number of hidden features in the KGAT model\n",
    "out_features = 1  # Number of output features in the KGAT model\n",
    "num_layers = 2  # Number of layers in the KGAT model\n",
    "\n",
    "numeric_keys = [\n",
    "    'acousticness', 'danceability', 'duration_ms', 'energy', 'explicit',\n",
    "    'liveness', 'loudness', 'popularity', 'speechiness', 'tempo', 'valence'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad68c738-7911-4b97-9589-b221f6447983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = KGAT(num_layers=num_layers, in_features=len(numeric_keys), hidden_features=hidden_features, out_features=out_features)\n",
    "loaded_model.load_state_dict(torch.load('trained_kgat_model_v100.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e0616d-ff3f-4bdf-87cd-18b0e2fc1fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# connect to MongoDB\n",
    "username = \"...\"\n",
    "password = \"...\"\n",
    "cluster_name = \"...\"\n",
    "dbname = \"tune-playlists\"\n",
    "client = MongoClient(f\"mongodb+srv://{username}:{password}@{cluster_name}.mongodb.net/{dbname}?retryWrites=true&w=majority\")\n",
    "db = client[dbname]\n",
    "collection = db[\"tune-users\"]\n",
    "\n",
    "# define the query\n",
    "query = { \"user_id\": \"...\" }\n",
    "\n",
    "result = collection.find(query, { \"top_tracks\": 1, \"_id\": 0 })\n",
    "\n",
    "track_ids = []\n",
    "\n",
    "for document in result:\n",
    "    for track in document['top_tracks']:\n",
    "        track_ids.append(track['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118607dc-9169-496d-ac82-effe73528589",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "required_features = [\n",
    "    't.acousticness', 't.danceability', 't.duration_ms', 't.energy', 't.explicit',\n",
    "    't.liveness', 't.loudness', 't.popularity', 't.speechiness', 't.tempo', 't.valence'\n",
    "]\n",
    "track_features = []\n",
    "for track in track_result:\n",
    "    x_features = [track[feature] for feature in required_features]\n",
    "    track_features.append(x_features)\n",
    "track_features_tensor = torch.FloatTensor(track_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17a703e-1ba7-4af8-9d85-0c384c22b1f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert adjacency matrices to PyTorch tensors\n",
    "shared_album_adj_tensor = torch.FloatTensor(shared_album_adj.toarray())\n",
    "shared_artist_adj_tensor = torch.FloatTensor(shared_artist_adj.toarray())\n",
    "shared_genre_adj_tensor = torch.FloatTensor(shared_genre_adj.toarray())\n",
    "cosine_similarity_adj_tensor = torch.FloatTensor(cosine_similarity_adj.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ba5fc4f-f6d9-4366-9016-50639918ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_adj_tensor = torch.FloatTensor(cosine_similarity_adj.toarray())\n",
    "def get_ranked_recommendations(user_songs, model, unique_track_ids, track_features):\n",
    "    # Move the model to the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create an input tensor containing user's input songs' features\n",
    "    user_song_indices = [unique_track_ids.index(track_id) for track_id in user_songs if track_id in unique_track_ids]\n",
    "    user_song_features = track_features[user_song_indices]\n",
    "\n",
    "    # Calculate the embeddings of the user's input songs using the trained model\n",
    "    user_song_embeddings = model(cosine_similarity_adj_tensor[:len(user_song_indices), :len(user_song_indices)], user_song_features)\n",
    "    user_song_embeddings += model(shared_album_adj_tensor[:len(user_song_indices), :len(user_song_indices)], user_song_features)\n",
    "    user_song_embeddings += model(shared_artist_adj_tensor[:len(user_song_indices), :len(user_song_indices)], user_song_features)\n",
    "    user_song_embeddings += model(shared_genre_adj_tensor[:len(user_song_indices), :len(user_song_indices)], user_song_features)\n",
    "    \n",
    "    recommendations = []\n",
    "    print(len(user_song_embeddings))\n",
    "    for tx_embedding in user_song_embeddings:\n",
    "        print(tx_embedding)\n",
    "        print(tx_embedding.size())\n",
    "        similarity_scores = torch.mm(track_embeddings, tx_embedding.t()).squeeze()\n",
    "        sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
    "        ranked_track_ids = [unique_track_ids[idx] for idx in sorted_indices.tolist()]\n",
    "        recommendations.append(ranked_track_ids[0])\n",
    "        \n",
    "    \n",
    "    # Calculate the average of the input songs' embeddings\n",
    "    #avg_embedding = torch.mean(user_song_embeddings, dim=0, keepdim=True)\n",
    "\n",
    "    # Calculate the similarity score between the average embedding and all tracks in the dataset\n",
    "    #similarity_scores = torch.mm(track_embeddings, avg_embedding.t()).squeeze()\n",
    "\n",
    "    # Sort similarity scores in descending order and get the corresponding indices\n",
    "    #sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
    "\n",
    "    # Convert the sorted indices to track IDs\n",
    "    #ranked_track_ids = [unique_track_ids[idx] for idx in sorted_indices.tolist()]\n",
    "\n",
    "    # Remove the user's input songs from the ranked recommendations\n",
    "    #recommendations = [track_id for track_id in ranked_track_ids if track_id not in user_songs]\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb46c934-49d2-431b-80ea-a3a4138422b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_recommendations(user_songs, model, unique_track_ids, track_features):\n",
    "    # Move the model to the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create an input tensor containing user's input songs' features\n",
    "    user_song_indices = [unique_track_ids.index(track_id) for track_id in user_songs if track_id in unique_track_ids]\n",
    "    user_song_features = track_features[user_song_indices]\n",
    "\n",
    "    # Calculate the embeddings of the user's input songs using the trained model\n",
    "    user_song_embeddings = model(cosine_similarity_adj_tensor[:len(user_song_indices), :len(user_song_indices)], user_song_features)\n",
    "    user_song_embeddings += model(shared_album_adj_tensor[:len(user_song_indices), :len(user_song_indices)], user_song_features)\n",
    "    user_song_embeddings += model(shared_artist_adj_tensor[:len(user_song_indices), :len(user_song_indices)], user_song_features)\n",
    "    user_song_embeddings += model(shared_genre_adj_tensor[:len(user_song_indices), :len(user_song_indices)], user_song_features)  \n",
    "    \n",
    "    # Calculate the average of the input songs' embeddings\n",
    "    avg_embedding = torch.mean(user_song_embeddings, dim=0, keepdim=True)\n",
    "\n",
    "    # Calculate the similarity score between the average embedding and all tracks in the dataset\n",
    "    similarity_scores = torch.mm(track_embeddings, avg_embedding.t()).squeeze()\n",
    "\n",
    "    # Sort similarity scores in descending order and get the corresponding indices\n",
    "    sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
    "\n",
    "    # Convert the sorted indices to track IDs\n",
    "    ranked_track_ids = [unique_track_ids[idx] for idx in sorted_indices.tolist()]\n",
    "\n",
    "    # Remove the user's input songs from the ranked recommendations\n",
    "    recommendations = [track_id for track_id in ranked_track_ids if track_id not in user_songs]\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b27126-5329-476b-8dad-1ae3d990fd2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ranked_recommendations(user_songs, model, unique_track_ids, track_features):\n",
    "    # Move the model to the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    all_recommendations = []\n",
    "    \n",
    "    for user_song in user_songs:\n",
    "        # Create an input tensor containing one of user's songs' features\n",
    "        user_song_index = unique_track_ids.index(user_song)\n",
    "        user_song_feature = track_features[user_song_index].unsqueeze(0)  # Add an extra dimension to match the input shape\n",
    "        \n",
    "        songX_tensor = torch.FloatTensor(user_song_feature)\n",
    "        \n",
    "        print(songX_tensor.size())\n",
    "        \n",
    "        # Calculate the embedding of the user's song using the trained model\n",
    "        user_song_embedding = model(cosine_similarity_adj_tensor, songX_tensor)\n",
    "        user_song_embedding += model(shared_album_adj_tensor[user_song_index], user_song_feature)\n",
    "        user_song_embedding += model(shared_artist_adj_tensor[user_song_index], user_song_feature)\n",
    "        user_song_embedding += model(shared_genre_adj_tensor[user_song_index], user_song_feature)\n",
    "        \n",
    "        # Calculate the similarity score between the user's song embedding and all tracks in the dataset\n",
    "        similarity_scores = torch.mm(track_embeddings, user_song_embedding.t()).squeeze()\n",
    "\n",
    "        # Sort similarity scores in descending order and get the corresponding indices\n",
    "        sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
    "\n",
    "        # Convert the sorted indices to track IDs\n",
    "        ranked_track_ids = [unique_track_ids[idx] for idx in sorted_indices.tolist()]\n",
    "\n",
    "        # Remove the user's song from the ranked recommendations\n",
    "        recommendations = [track_id for track_id in ranked_track_ids if track_id not in user_songs]\n",
    "        \n",
    "        all_recommendations.append(recommendations)\n",
    "\n",
    "    return all_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cbc31a1a-d4f2-479b-b870-a7a8fa843d85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert the adjacency matrices to PyTorch tensors\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m shared_album_adj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mshared_album_adj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m())\n\u001b[1;32m      5\u001b[0m shared_artist_adj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(shared_artist_adj\u001b[38;5;241m.\u001b[39mtoarray())\n\u001b[1;32m      6\u001b[0m shared_genre_adj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(shared_genre_adj\u001b[38;5;241m.\u001b[39mtoarray())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert the adjacency matrices to PyTorch tensors\n",
    "shared_album_adj = torch.from_numpy(shared_album_adj.toarray())\n",
    "shared_artist_adj = torch.from_numpy(shared_artist_adj.toarray())\n",
    "shared_genre_adj = torch.from_numpy(shared_genre_adj.toarray())\n",
    "cosine_similarity_adj = torch.from_umpy(cosine_similarity_adj.toarray())\n",
    "\n",
    "# Add a new dimension to each tensor to represent the number of matrices\n",
    "shared_album_adj = shared_album_adj.unsqueeze(0)\n",
    "shared_artist_adj = shared_artist_adj.unsqueeze(0)\n",
    "shared_genre_adj = shared_genre_adj.unsqueeze(0)\n",
    "cosine_similarity_adj = cosine_similarity_adj.unsqueeze(0)\n",
    "\n",
    "# Stack the adjacency matrices into a single tensor\n",
    "adj_matrix = torch.cat([shared_album_adj, shared_artist_adj, shared_genre_adj, cosine_similarity_adj], dim=0)\n",
    "\n",
    "# Reshape the tensor to match the input shape of the KGAT model\n",
    "adj_matrix = adj_matrix.unsqueeze(0)  # Add extra dimension for batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5dd89156-dc69-4fca-9f78-d1a84e7e3f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26817, 26817])\n",
      "\n",
      "torch.Size([26817, 11])\n"
     ]
    }
   ],
   "source": [
    "print(shared_album_adj_tensor.size())\n",
    "print()\n",
    "print(track_features_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52e4ba-8028-4184-aeff-e977cc3fafa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e99a398f-b2ac-4686-8112-ca5ea48ab115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26817, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d00c21d-7adb-4fc5-9bbc-5031e10e6504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (26852x26852 and 1x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m \u001b[43mget_ranked_recommendations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcosine_similarity_adj_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_track_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_features_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mget_ranked_recommendations\u001b[0;34m(cosine_similarity_adj_tensor, user_songs, model, unique_track_ids, track_features)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(songX_tensor\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate the embedding of the user's song using the trained model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m user_song_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcosine_similarity_adj_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msongX_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m user_song_embedding \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model(shared_album_adj_tensor[user_song_index], user_song_feature)\n\u001b[1;32m     19\u001b[0m user_song_embedding \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model(shared_artist_adj_tensor[user_song_index], user_song_feature)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m, in \u001b[0;36mKGAT.forward\u001b[0;34m(self, adjacency_matrix, input_features)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, adjacency_matrix, input_features):\n\u001b[0;32m---> 53\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Add an extra dimension for Batch Normalization\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_bn(x))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mKGATLayer.forward\u001b[0;34m(self, adjacency_matrix, input_features)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, adjacency_matrix, input_features):\n\u001b[1;32m     28\u001b[0m     output_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(input_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (26852x26852 and 1x64)"
     ]
    }
   ],
   "source": [
    "recommendations = get_ranked_recommendations(cosine_similarity_adj_tensor, track_ids, loaded_model, unique_track_ids, track_features_tensor)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29c62dfb-c856-435e-8faf-1fb57433f717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3B3A4mqiRhspnA3VCyfEbb',\n",
       " '36cG8YmyzibJ4WKfEwAn9d',\n",
       " '41fe96o3cxCfvINxcxEe4R',\n",
       " '72GQG5AyjaZBDlbWUOw7Xb',\n",
       " '1xrSV0FyGg77IuJGYF3siH',\n",
       " '7BDFu1LVCAe0yZDWcF5d7P',\n",
       " '52PCi5DlwcHFknaMyEE9cD',\n",
       " '5kNbpvJ5b6R9Zqfm6c9sIX',\n",
       " '2acK24b60RQD2zBpW0Zsrw',\n",
       " '2kTRzrl2zO2ghx1EBRT9lP',\n",
       " '1vNNfTgHsrpOXeiaXQBlH7',\n",
       " '6dgKHNwngP42Iww8HOsSH6',\n",
       " '4qGK21F2n8zYRNsllinNaS',\n",
       " '45XgvZ2ggPKibnPGtduIGY',\n",
       " '1jss21gv4BPA8FZv2VNDuV',\n",
       " '5Dvd2vfqvPg0RCXQ9DpKee',\n",
       " '00bq71I9OZ87M6jZqQFV6I',\n",
       " '3em4Dn6hxzdgjKpLgMZSy1',\n",
       " '6btkdvumrTmcFzy3oFpZqS',\n",
       " '1Gz3r6XFSqYdqBhyYaHtto',\n",
       " '0SrLz2zZUMYvYuiKaMaq1I',\n",
       " '7tQyxRUBcy0XVZbATedxBU',\n",
       " '1nV6VafLPuRSsXgbDY3i6L',\n",
       " '5LghBGmNUUwI2BvX2boIdK',\n",
       " '3Cl8IGVb7E63EDIXTn5Pk7',\n",
       " '7fj9sa5dPVayB0YtsAX9Kb',\n",
       " '4BIuY0oEopXizyP3WvkNrT',\n",
       " '175Xa0mLfc90AlFMHMfE9O',\n",
       " '08DqG1RqpxnICWOdOp2PLV',\n",
       " '0SPAmRhrRUcBvDV4uctdcx',\n",
       " '5osHRqrBmK2Am2FhoNm2FL',\n",
       " '53hco57B1r9b2HPHGek4qu',\n",
       " '7MA8T4pKN6VeBIEGE9hggN',\n",
       " '5OWt21Fcte8qlNuf9r0fVI',\n",
       " '0tAFy8LVdMQgW6A6Nq7olS',\n",
       " '4FKRT4uPFx2L4exy9DCs1o',\n",
       " '1unMIRKio7PpOxPWRDMtRc',\n",
       " '3F5CgOj3wFlRv51JsHbxhe',\n",
       " '7AX4Llf4P6TMTR6Zsv3oB1',\n",
       " '4m4aKJv0PN3Hz7BrwIlRYO',\n",
       " '0EvhH70UAoakZInIqmZLVV',\n",
       " '2jWPHVYIUa2Q8DrwFioX1C',\n",
       " '6tYzs9xlEeYznMJpJ0sUkI',\n",
       " '6eIEJLVaPTU3NuctqdWCzJ',\n",
       " '5tNjmRhe7peRo84dfgulVA',\n",
       " '4RWMUN7m78hYp4nvXVCxzJ',\n",
       " '56Qg15afWhWx83aFusmXc4',\n",
       " '7mXNYEVh9FW72c12qBaO3p',\n",
       " '06XuHBFQejxy9jhNBaHEwx',\n",
       " '6ygmkY26MvwW839qaWg4jL']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10ec113c-db08-4b95-8d5a-0a26e3df5d06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track name: Acapella\n",
      "Artist(s): Kelis\n",
      "\n",
      "Track name: Sunworshipper\n",
      "Artist(s): Mylo\n",
      "\n",
      "Track name: Anything You Want (Not That)\n",
      "Artist(s): Belleruche\n",
      "\n",
      "Track name: Cannot Contain This - Radio Edit\n",
      "Artist(s): Moloko\n",
      "\n",
      "Track name: At Night\n",
      "Artist(s): Shakedown\n",
      "\n",
      "Track name: Groovejet\n",
      "Artist(s): Spiller\n",
      "\n",
      "Track name: Scratched\n",
      "Artist(s): Étienne de Crécy\n",
      "\n",
      "Track name: Strict Machine\n",
      "Artist(s): Goldfrapp\n",
      "\n",
      "Track name: Blind - Radio Edit\n",
      "Artist(s): Hercules & Love Affair\n",
      "\n",
      "Track name: Summer's Here\n",
      "Artist(s): Magnus\n",
      "\n",
      "Track name: Safe From Harm - 2012 Mix/Master\n",
      "Artist(s): Massive Attack\n",
      "\n",
      "Track name: Central Reservation - Spiritual Life - Ibadan Edit\n",
      "Artist(s): Beth Orton, Joaquin \"Joe\" Claussell, Jerome Sydenham\n",
      "\n",
      "Track name: I Feel Better\n",
      "Artist(s): Hot Chip\n",
      "\n",
      "Track name: Over & Over\n",
      "Artist(s): Moloko\n",
      "\n",
      "Track name: Invisible Light\n",
      "Artist(s): Scissor Sisters\n",
      "\n",
      "Track name: just wanna luv\n",
      "Artist(s): Conro\n",
      "\n",
      "Track name: You and Me\n",
      "Artist(s): The Magician\n",
      "\n",
      "Track name: Without You - Mesto Remix\n",
      "Artist(s): Mike Williams, Felix Jaehn, Jordan Shaw, Mesto\n",
      "\n",
      "Track name: Thief - Flux Pavilion Remix\n",
      "Artist(s): Ookay, Flux Pavilion\n",
      "\n",
      "Track name: By My Side\n",
      "Artist(s): N3WPORT\n",
      "\n",
      "Track name: I Won't Let You Fall (feat. Finn Askew) - Hermitude Remix\n",
      "Artist(s): WHIPPED CREAM, Hermitude, Finn Askew\n",
      "\n",
      "Track name: Haven\n",
      "Artist(s): William Black, Dia Frampton\n",
      "\n",
      "Track name: One Last Dance\n",
      "Artist(s): Audien, XIRA\n",
      "\n",
      "Track name: Bruuuh (with Denzel Curry) - Remix\n",
      "Artist(s): JID, Denzel Curry\n",
      "\n",
      "Track name: Don’t Go Mad (feat. Seinabo Sey)\n",
      "Artist(s): Swedish House Mafia, Seinabo Sey\n",
      "\n",
      "Track name: All We Need - Dzeko & Torres Remix Radio Edit\n",
      "Artist(s): ODESZA, Shy Girls, Dzeko & Torres\n",
      "\n",
      "Track name: Oh shit…are we in love?\n",
      "Artist(s): Valley\n",
      "\n",
      "Track name: Superhuman\n",
      "Artist(s): Tritonal, Codeko\n",
      "\n",
      "Track name: Crime (with Skott)\n",
      "Artist(s): Grey, Skott\n",
      "\n",
      "Track name: The Last Goodbye\n",
      "Artist(s): ODESZA, Bettye LaVette\n",
      "\n",
      "Track name: Daydreamer (feat. Example) - Radio Edit\n",
      "Artist(s): Flux Pavilion, Example\n",
      "\n",
      "Track name: Monument - Kasbo Remix\n",
      "Artist(s): Mutemath, Kasbo\n",
      "\n",
      "Track name: Limitless\n",
      "Artist(s): Martin Garrix, Mesto\n",
      "\n",
      "Track name: Open Road (feat. Rico & Miella)\n",
      "Artist(s): Trivecta, Rico & Miella\n",
      "\n",
      "Track name: Do You?\n",
      "Artist(s): TroyBoi\n",
      "\n",
      "Track name: dashstar* - VIP\n",
      "Artist(s): Knock2\n",
      "\n",
      "Track name: French The Kid x Fumez The Engineer - Plugged In Part 1\n",
      "Artist(s): Fumez The Engineer, French The Kid\n",
      "\n",
      "Track name: Jimmy Cooks (feat. 21 Savage)\n",
      "Artist(s): Drake, 21 Savage\n",
      "\n",
      "Track name: yours\n",
      "Artist(s): jaakob\n",
      "\n",
      "Track name: Rollin - Bonus Track\n",
      "Artist(s): G Herbo\n",
      "\n",
      "Track name: Drop Top\n",
      "Artist(s): TVBOO, Runnit\n",
      "\n",
      "Track name: 6 ft.\n",
      "Artist(s): Vindata\n",
      "\n",
      "Track name: After Hours\n",
      "Artist(s): charlieonnafriday\n",
      "\n",
      "Track name: Dunno - Recorded at Spotify Studios NYC\n",
      "Artist(s): Mac Miller\n",
      "\n",
      "Track name: Cake Mints\n",
      "Artist(s): LYNY\n",
      "\n",
      "Track name: Dreams\n",
      "Artist(s): Anamanaguchi, Flux Pavilion\n",
      "\n",
      "Track name: Sirens (feat. Caroline Polachek)\n",
      "Artist(s): Flume, Caroline Polachek\n",
      "\n",
      "Track name: Only You\n",
      "Artist(s): Alesso, Sentinel\n",
      "\n",
      "Track name: 1 Night (feat. Charli XCX)\n",
      "Artist(s): Mura Masa, Charli XCX\n",
      "\n",
      "Track name: Heaven (feat. Maty Noyes) [Lenno Remix]\n",
      "Artist(s): Audien, Maty Noyes, Lenno\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import spotipy\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id='...', client_secret=\"...\")\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "def get_track_info(track_id):\n",
    "    track_info = sp.track(track_id)\n",
    "    track_name = track_info['name']\n",
    "    artist_info = track_info['artists']\n",
    "    artist_names = [artist['name'] for artist in artist_info]\n",
    "    return track_name, artist_names\n",
    "\n",
    "for track_id in recommendations[-50:]:\n",
    "    track_name, artist_names = get_track_info(track_id)\n",
    "    print(f'Track name: {track_name}')\n",
    "    print(f'Artist(s): {\", \".join(artist_names)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa85ebf-aba9-4cac-a4e8-99b3500dde0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f6600-42f2-4049-b594-b067f38d9382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
